Moral reasoning is a complex cognitive process that intersects with philosophy, psychology, and neuroscience. In my research, I focus on how people make moral decisions in real-world contexts—not just abstract dilemmas, but situations involving competing values, ambiguity, and time pressure. There’s a long-standing debate between deontological and consequentialist reasoning—rules versus outcomes. But most people don’t neatly follow one or the other. We’ve found through fMRI and behavioral studies that emotional salience plays a huge role. The amygdala lights up when people face personal moral dilemmas, while the prefrontal cortex is more active in abstract or impersonal cases. We’re also investigating how moral intuitions develop. Developmental research suggests that fairness and harm aversion emerge early, but cultural learning shapes how those get expressed. In collectivist cultures, loyalty and respect for authority carry more moral weight. That suggests that moral cognition is both universal and culturally malleable. One of the most promising approaches now is computational modeling. We use Bayesian inference to simulate how agents update moral beliefs over time, especially when they encounter moral conflict or social feedback. We also look at moral learning in artificial agents—how to encode value systems that align with human ethical intuitions. I’m also really interested in moral injury—the psychological consequences of violating one’s own moral code, often in military or medical contexts. This challenges the idea that morality is just about judgment; it's about identity, emotion, and trauma too. As we face global ethical challenges—AI, climate justice, biomedical enhancement—we need better models of how people reason morally under uncertainty, and how we can foster cross-cultural moral dialogue.