Artificial intelligence is no longer a niche field—it’s a driving force behind everything from medicine to logistics to creative content generation. In my work, I focus on representation learning, especially how deep neural networks abstract and encode complex input data into latent spaces. We’ve been particularly interested in how self-supervised learning is reshaping the field. These models, trained without labeled data, are showing astonishing generalization across domains. But this raises new questions about interpretability, robustness, and alignment with human intent. We use techniques like probing classifiers and attention attribution to reverse-engineer what these networks are actually 'paying attention to.' But there's still a massive gap between what these systems do and how we understand it. That’s why there’s growing interest in neuro-symbolic approaches and causal modeling. One challenge we keep facing is that large models can perform well statistically while failing in unpredictable ways. So we’re investing more in uncertainty estimation and out-of-distribution detection. We need AI systems that know when they don’t know. And the ethics are enormous. From bias and fairness to explainability and accountability, the societal stakes are high. We’re working with ethicists, lawyers, and educators to ensure that our systems are not just technically impressive but socially responsible.